{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Implementaión de la Q-Table\n",
        "\n",
        "### Problema: Agente en un Laberinto\n",
        "El agente se encuentra en un entorno de cuadrícula (5x5) y debe aprender a llegar a una meta. La recompensa se otorga cuando llega a la meta, y hay penalizaciones por cada paso que da.\n",
        "\n",
        "Entorno (Grid World):\n",
        "El entorno es una cuadrícula en la que:\n",
        "\n",
        "- El agente comienza en la posición (0, 0).\n",
        "- La meta está en la posición (4, 4).\n",
        "- Cada acción tiene un costo de -1 para incentivar llegar rápidamente a la meta.\n",
        "- Al llegar a la meta, el agente recibe una recompensa de +100.\n",
        "\n",
        "Acciones:\n",
        "El agente tiene 4 posibles acciones:\n",
        "\n",
        "- Arriba\n",
        "- Abajo\n",
        "- Izquierda\n",
        "- Derecha"
      ],
      "metadata": {
        "id": "i6_Lbm4Eqw97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Parámetros del entorno\n",
        "rows, cols = 5, 5  # Tamaño del entorno 5x5\n",
        "goal = (4, 4)      # La meta está en la posición (4, 4)\n",
        "\n",
        "# Parámetros del Q-Learning\n",
        "alpha = 0.1  # Tasa de aprendizaje\n",
        "gamma = 0.9  # Factor de descuento\n",
        "epsilon = 0.4  # Parámetro de exploración (probabilidad de tomar acción aleatoria)\n",
        "num_episodes = 50  # Número de episodios para entrenar\n",
        "\n",
        "# Inicializar la tabla Q con ceros\n",
        "Q_table = np.zeros((rows, cols, 4))  # 4 acciones: arriba, abajo, izquierda, derecha\n",
        "\n",
        "# Función para elegir la acción (política epsilon-greedy)\n",
        "def choose_action(state):\n",
        "    if random.uniform(0, 1) < epsilon:  # Exploración\n",
        "        return random.choice([0, 1, 2, 3])  # Arriba, Abajo, Izquierda, Derecha\n",
        "    else:  # Explotación\n",
        "        return np.argmax(Q_table[state[0], state[1]])\n",
        "\n",
        "# Función para realizar la acción y actualizar el estado\n",
        "def take_action(state, action):\n",
        "    row, col = state\n",
        "    if action == 0 and row > 0:         # Arriba\n",
        "        row -= 1\n",
        "    elif action == 1 and row < rows - 1:  # Abajo\n",
        "        row += 1\n",
        "    elif action == 2 and col > 0:       # Izquierda\n",
        "        col -= 1\n",
        "    elif action == 3 and col < cols - 1:  # Derecha\n",
        "        col += 1\n",
        "    return (row, col)\n",
        "\n",
        "# Función de entrenamiento usando Q-Learning\n",
        "for episode in range(num_episodes):\n",
        "    state = (0, 0)  # El agente siempre empieza en (0, 0)\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state)  # Escoge una acción\n",
        "\n",
        "        next_state = take_action(state, action)  # Mueve al agente\n",
        "        reward = -1  # Penalización por movimiento\n",
        "\n",
        "        if next_state == goal:  # Si el agente llega a la meta\n",
        "            reward = 100\n",
        "            done = True\n",
        "\n",
        "        # Actualización de la tabla Q\n",
        "        old_value = Q_table[state[0], state[1], action]\n",
        "        next_max = np.max(Q_table[next_state[0], next_state[1]])\n",
        "\n",
        "        # Fórmula de actualización de Q\n",
        "        Q_table[state[0], state[1], action] = old_value + alpha * (reward + gamma * next_max - old_value)\n",
        "\n",
        "        state = next_state  # Mover al agente al siguiente estado\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        print(f\"{state}\", end=' ')\n",
        "\n",
        "    # Al final de cada episodio, se puede imprimir el progreso\n",
        "    print(f\"\\nEpisodio {episode + 1}, Recompensa total: {total_reward}, Pasos: {steps}\")\n",
        "\n",
        "# Mostrar la tabla Q final\n",
        "print(\"Tabla Q entrenada:\")\n",
        "print(Q_table)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkvIYaP6q00r",
        "outputId": "8ebaca09-e456-4165-da08-18e511537e66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 0) (1, 0) (1, 0) (0, 0) (0, 0) (1, 0) (0, 0) (0, 1) (0, 2) (0, 2) (1, 2) (1, 3) (0, 3) (0, 3) (1, 3) (2, 3) (2, 4) (1, 4) (1, 4) (0, 4) (0, 4) (1, 4) (2, 4) (3, 4) (2, 4) (2, 3) (1, 3) (2, 3) (3, 3) (4, 3) (3, 3) (2, 3) (2, 2) (2, 1) (1, 1) (0, 1) (0, 1) (1, 1) (2, 1) (3, 1) (2, 1) (2, 0) (1, 0) (2, 0) (3, 0) (2, 0) (2, 0) (2, 1) (2, 2) (2, 3) (1, 3) (1, 2) (0, 2) (0, 1) (0, 0) (1, 0) (1, 0) (1, 1) (1, 0) (1, 0) (0, 0) (0, 1) (0, 1) (1, 1) (1, 2) (1, 1) (0, 1) (0, 2) (0, 3) (0, 2) (0, 3) (0, 4) (0, 3) (0, 3) (1, 3) (1, 4) (0, 4) (0, 4) (0, 3) (0, 4) (0, 4) (1, 4) (1, 3) (1, 2) (2, 2) (1, 2) (1, 1) (2, 1) (1, 1) (2, 1) (2, 2) (3, 2) (2, 2) (3, 2) (4, 2) (3, 2) (3, 1) (3, 0) (4, 0) (3, 0) (3, 1) (4, 1) (3, 1) (3, 2) (3, 3) (3, 2) (4, 2) (4, 2) (4, 1) (4, 1) (4, 0) (4, 0) (4, 0) (4, 0) (4, 0) (4, 1) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 1, Recompensa total: -18, Pasos: 119\n",
            "(0, 1) (0, 0) (0, 0) (0, 0) (0, 0) (0, 0) (0, 0) (1, 0) (2, 0) (1, 0) (1, 1) (1, 2) (1, 3) (0, 3) (0, 4) (0, 4) (0, 3) (0, 3) (1, 3) (1, 4) (2, 4) (2, 4) (1, 4) (1, 4) (1, 3) (2, 3) (3, 3) (3, 2) (3, 1) (2, 1) (2, 2) (2, 1) (3, 1) (4, 1) (3, 1) (3, 0) (3, 0) (2, 0) (3, 0) (4, 0) (3, 0) (3, 0) (4, 0) (4, 0) (4, 0) (4, 1) (4, 2) (3, 2) (3, 3) (3, 4) (4, 4) \n",
            "Episodio 2, Recompensa total: 50, Pasos: 51\n",
            "(0, 1) (1, 1) (1, 0) (2, 0) (1, 0) (2, 0) (2, 0) (2, 1) (2, 0) (2, 0) (2, 0) (3, 0) (3, 0) (3, 1) (3, 2) (2, 2) (1, 2) (0, 2) (0, 2) (0, 1) (0, 2) (1, 2) (1, 1) (0, 1) (0, 1) (0, 2) (0, 3) (0, 2) (0, 2) (1, 2) (0, 2) (0, 1) (0, 0) (0, 0) (1, 0) (0, 0) (0, 1) (1, 1) (1, 2) (2, 2) (1, 2) (1, 3) (0, 3) (0, 2) (0, 3) (1, 3) (1, 2) (2, 2) (2, 1) (1, 1) (1, 0) (1, 1) (2, 1) (3, 1) (3, 0) (2, 0) (2, 1) (2, 0) (3, 0) (3, 1) (2, 1) (2, 2) (3, 2) (4, 2) (4, 2) (4, 2) (4, 1) (4, 1) (3, 1) (2, 1) (2, 0) (1, 0) (1, 0) (1, 1) (1, 2) (2, 2) (1, 2) (1, 3) (0, 3) (0, 2) (1, 2) (1, 1) (0, 1) (0, 1) (0, 0) (0, 0) (0, 0) (1, 0) (2, 0) (2, 1) (3, 1) (4, 1) (4, 1) (4, 0) (3, 0) (4, 0) (4, 1) (3, 1) (3, 2) (3, 3) (4, 3) (4, 2) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 3, Recompensa total: -4, Pasos: 105\n",
            "(0, 1) (0, 2) (0, 2) (0, 1) (1, 1) (1, 0) (1, 0) (0, 0) (0, 0) (0, 0) (1, 0) (1, 0) (1, 1) (0, 1) (0, 1) (0, 0) (0, 1) (0, 0) (0, 0) (0, 0) (0, 0) (1, 0) (2, 0) (3, 0) (3, 0) (2, 0) (2, 0) (1, 0) (1, 0) (0, 0) (0, 1) (0, 2) (0, 3) (0, 4) (0, 4) (0, 4) (0, 4) (1, 4) (0, 4) (0, 4) (0, 4) (1, 4) (2, 4) (3, 4) (2, 4) (3, 4) (3, 3) (4, 3) (4, 3) (4, 4) \n",
            "Episodio 4, Recompensa total: 51, Pasos: 50\n",
            "(0, 0) (0, 0) (1, 0) (1, 1) (1, 2) (0, 2) (1, 2) (2, 2) (2, 1) (1, 1) (2, 1) (3, 1) (4, 1) (4, 2) (3, 2) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 5, Recompensa total: 83, Pasos: 18\n",
            "(0, 1) (1, 1) (1, 0) (0, 0) (0, 0) (1, 0) (1, 1) (2, 1) (2, 2) (2, 3) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 6, Recompensa total: 86, Pasos: 15\n",
            "(0, 1) (0, 1) (0, 0) (1, 0) (2, 0) (3, 0) (3, 0) (3, 1) (3, 0) (4, 0) (4, 0) (4, 0) (4, 1) (4, 0) (3, 0) (2, 0) (2, 1) (1, 1) (1, 2) (1, 3) (1, 4) (1, 3) (2, 3) (2, 4) (3, 4) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 7, Recompensa total: 73, Pasos: 28\n",
            "(0, 1) (0, 1) (0, 2) (0, 2) (0, 1) (1, 1) (0, 1) (0, 2) (0, 3) (0, 3) (0, 2) (0, 1) (1, 1) (1, 0) (0, 0) (0, 0) (0, 0) (1, 0) (2, 0) (2, 0) (2, 1) (2, 2) (2, 1) (2, 0) (1, 0) (2, 0) (3, 0) (4, 0) (4, 0) (4, 0) (3, 0) (3, 0) (3, 1) (2, 1) (2, 0) (3, 0) (3, 0) (3, 1) (3, 2) (2, 2) (2, 1) (3, 1) (4, 1) (4, 0) (4, 0) (4, 0) (3, 0) (2, 0) (2, 0) (2, 1) (1, 1) (2, 1) (2, 2) (3, 2) (3, 1) (4, 1) (4, 0) (4, 0) (4, 1) (4, 2) (4, 2) (4, 3) (3, 3) (2, 3) (1, 3) (1, 2) (1, 1) (1, 2) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 8, Recompensa total: 26, Pasos: 75\n",
            "(0, 1) (0, 1) (0, 2) (0, 2) (0, 2) (1, 2) (1, 1) (0, 1) (0, 2) (0, 2) (0, 3) (0, 4) (0, 3) (1, 3) (1, 4) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 9, Recompensa total: 82, Pasos: 19\n",
            "(0, 0) (0, 0) (0, 0) (0, 0) (1, 0) (1, 1) (2, 1) (3, 1) (3, 0) (3, 1) (3, 2) (2, 2) (2, 3) (2, 2) (2, 1) (2, 0) (2, 1) (2, 2) (3, 2) (2, 2) (2, 1) (1, 1) (1, 0) (1, 0) (1, 0) (1, 1) (0, 1) (1, 1) (1, 2) (1, 3) (2, 3) (2, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 10, Recompensa total: 66, Pasos: 35\n",
            "(0, 1) (0, 0) (0, 0) (0, 1) (1, 1) (1, 2) (1, 1) (1, 2) (0, 2) (0, 1) (0, 1) (0, 2) (0, 3) (0, 3) (0, 3) (1, 3) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 11, Recompensa total: 81, Pasos: 20\n",
            "(1, 0) (2, 0) (1, 0) (1, 0) (2, 0) (1, 0) (0, 0) (0, 1) (0, 0) (0, 1) (0, 1) (1, 1) (2, 1) (1, 1) (2, 1) (3, 1) (3, 2) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 12, Recompensa total: 81, Pasos: 20\n",
            "(1, 0) (2, 0) (2, 0) (3, 0) (3, 1) (2, 1) (2, 2) (1, 2) (0, 2) (0, 3) (0, 3) (0, 4) (0, 3) (0, 2) (1, 2) (1, 3) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 13, Recompensa total: 79, Pasos: 22\n",
            "(0, 0) (1, 0) (1, 0) (1, 0) (1, 0) (1, 1) (1, 0) (1, 1) (0, 1) (0, 2) (0, 3) (0, 4) (1, 4) (0, 4) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 14, Recompensa total: 83, Pasos: 18\n",
            "(0, 1) (0, 1) (1, 1) (2, 1) (2, 0) (2, 1) (3, 1) (2, 1) (3, 1) (3, 0) (2, 0) (1, 0) (0, 0) (0, 1) (0, 0) (1, 0) (1, 0) (1, 1) (1, 0) (2, 0) (2, 0) (3, 0) (4, 0) (4, 1) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 15, Recompensa total: 74, Pasos: 27\n",
            "(1, 0) (0, 0) (0, 0) (0, 0) (0, 0) (0, 1) (0, 2) (0, 1) (0, 1) (1, 1) (1, 2) (2, 2) (2, 3) (1, 3) (0, 3) (0, 4) (0, 3) (1, 3) (2, 3) (2, 2) (2, 3) (2, 4) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 16, Recompensa total: 75, Pasos: 26\n",
            "(0, 0) (1, 0) (2, 0) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 17, Recompensa total: 92, Pasos: 9\n",
            "(0, 0) (0, 0) (0, 1) (1, 1) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (2, 4) (2, 4) (2, 4) (2, 4) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 18, Recompensa total: 84, Pasos: 17\n",
            "(1, 0) (0, 0) (0, 0) (1, 0) (1, 1) (0, 1) (1, 1) (1, 2) (1, 3) (1, 2) (2, 2) (3, 2) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 19, Recompensa total: 86, Pasos: 15\n",
            "(0, 1) (0, 2) (0, 3) (0, 3) (0, 4) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 20, Recompensa total: 92, Pasos: 9\n",
            "(0, 0) (0, 1) (0, 0) (1, 0) (2, 0) (2, 0) (3, 0) (4, 0) (4, 1) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 21, Recompensa total: 89, Pasos: 12\n",
            "(1, 0) (2, 0) (2, 0) (3, 0) (3, 0) (3, 1) (4, 1) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 22, Recompensa total: 91, Pasos: 10\n",
            "(0, 0) (0, 0) (0, 0) (0, 0) (0, 1) (0, 2) (0, 3) (0, 2) (0, 1) (0, 1) (1, 1) (1, 0) (1, 1) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 23, Recompensa total: 82, Pasos: 19\n",
            "(1, 0) (1, 1) (0, 1) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 24, Recompensa total: 90, Pasos: 11\n",
            "(0, 1) (0, 1) (0, 0) (0, 0) (1, 0) (1, 0) (1, 0) (0, 0) (0, 1) (0, 2) (0, 3) (0, 2) (0, 1) (0, 0) (0, 0) (1, 0) (2, 0) (1, 0) (1, 1) (2, 1) (3, 1) (3, 2) (3, 3) (3, 4) (3, 3) (4, 3) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 25, Recompensa total: 72, Pasos: 29\n",
            "(0, 1) (0, 2) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 26, Recompensa total: 92, Pasos: 9\n",
            "(1, 0) (2, 0) (2, 1) (1, 1) (1, 0) (1, 1) (1, 0) (0, 0) (0, 1) (1, 1) (1, 2) (1, 3) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 27, Recompensa total: 85, Pasos: 16\n",
            "(0, 0) (0, 1) (0, 1) (0, 2) (0, 3) (0, 2) (0, 2) (0, 1) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 28, Recompensa total: 85, Pasos: 16\n",
            "(1, 0) (1, 0) (2, 0) (2, 0) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 29, Recompensa total: 91, Pasos: 10\n",
            "(0, 0) (1, 0) (1, 1) (1, 2) (2, 2) (2, 3) (3, 3) (4, 3) (4, 3) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 30, Recompensa total: 89, Pasos: 12\n",
            "(1, 0) (1, 0) (2, 0) (3, 0) (2, 0) (1, 0) (0, 0) (0, 0) (0, 1) (1, 1) (1, 2) (0, 2) (0, 2) (1, 2) (1, 1) (2, 1) (1, 1) (1, 2) (2, 2) (1, 2) (1, 3) (1, 4) (1, 3) (2, 3) (3, 3) (4, 3) (4, 2) (4, 3) (4, 2) (4, 3) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 31, Recompensa total: 68, Pasos: 33\n",
            "(0, 0) (0, 1) (0, 1) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 32, Recompensa total: 91, Pasos: 10\n",
            "(0, 1) (1, 1) (1, 2) (2, 2) (2, 1) (1, 1) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 33, Recompensa total: 89, Pasos: 12\n",
            "(0, 1) (0, 2) (0, 1) (0, 1) (0, 0) (1, 0) (2, 0) (2, 0) (3, 0) (4, 0) (4, 0) (4, 1) (3, 1) (4, 1) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 34, Recompensa total: 84, Pasos: 17\n",
            "(0, 0) (1, 0) (1, 1) (1, 2) (2, 2) (2, 3) (2, 4) (1, 4) (1, 3) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 35, Recompensa total: 88, Pasos: 13\n",
            "(0, 0) (0, 0) (0, 1) (0, 2) (1, 2) (2, 2) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 36, Recompensa total: 89, Pasos: 12\n",
            "(1, 0) (1, 1) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 37, Recompensa total: 91, Pasos: 10\n",
            "(0, 0) (0, 1) (1, 1) (1, 2) (2, 2) (3, 2) (4, 2) (4, 3) (4, 2) (4, 3) (4, 2) (4, 3) (4, 4) \n",
            "Episodio 38, Recompensa total: 88, Pasos: 13\n",
            "(1, 0) (2, 0) (2, 1) (1, 1) (1, 2) (2, 2) (1, 2) (2, 2) (2, 3) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 39, Recompensa total: 89, Pasos: 12\n",
            "(0, 1) (0, 1) (1, 1) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (3, 4) (4, 4) \n",
            "Episodio 40, Recompensa total: 91, Pasos: 10\n",
            "(1, 0) (1, 1) (2, 1) (2, 2) (2, 1) (2, 2) (2, 3) (2, 4) (3, 4) (3, 4) (4, 4) \n",
            "Episodio 41, Recompensa total: 90, Pasos: 11\n",
            "(0, 1) (0, 2) (0, 3) (1, 3) (2, 3) (2, 4) (3, 4) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 42, Recompensa total: 91, Pasos: 10\n",
            "(1, 0) (1, 1) (1, 2) (0, 2) (1, 2) (2, 2) (2, 3) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 43, Recompensa total: 91, Pasos: 10\n",
            "(0, 0) (1, 0) (1, 0) (1, 1) (1, 2) (2, 2) (2, 3) (2, 4) (1, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 44, Recompensa total: 89, Pasos: 12\n",
            "(0, 1) (1, 1) (0, 1) (1, 1) (0, 1) (1, 1) (1, 0) (1, 1) (1, 2) (1, 3) (2, 3) (2, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 45, Recompensa total: 86, Pasos: 15\n",
            "(0, 0) (0, 1) (1, 1) (1, 2) (1, 3) (2, 3) (2, 4) (2, 3) (1, 3) (1, 2) (2, 2) (3, 2) (4, 2) (4, 3) (3, 3) (2, 3) (3, 3) (4, 3) (4, 3) (4, 4) \n",
            "Episodio 46, Recompensa total: 81, Pasos: 20\n",
            "(0, 1) (1, 1) (1, 2) (2, 2) (2, 3) (3, 3) (4, 3) (4, 3) (4, 4) \n",
            "Episodio 47, Recompensa total: 92, Pasos: 9\n",
            "(0, 1) (1, 1) (1, 2) (1, 3) (0, 3) (0, 3) (1, 3) (2, 3) (3, 3) (2, 3) (1, 3) (1, 2) (0, 2) (0, 2) (0, 1) (1, 1) (2, 1) (2, 2) (2, 3) (2, 4) (2, 4) (3, 4) (3, 3) (2, 3) (1, 3) (1, 4) (2, 4) (3, 4) (3, 4) (4, 4) \n",
            "Episodio 48, Recompensa total: 71, Pasos: 30\n",
            "(0, 1) (0, 1) (0, 1) (1, 1) (0, 1) (0, 2) (1, 2) (2, 2) (3, 2) (3, 3) (4, 3) (4, 4) \n",
            "Episodio 49, Recompensa total: 89, Pasos: 12\n",
            "(0, 0) (0, 0) (0, 1) (0, 2) (1, 2) (1, 1) (1, 2) (2, 2) (2, 3) (2, 2) (2, 3) (2, 4) (3, 4) (3, 4) (3, 4) (2, 4) (3, 4) (4, 4) \n",
            "Episodio 50, Recompensa total: 83, Pasos: 18\n",
            "Tabla Q entrenada:\n",
            "[[[-2.19890675e+00 -2.13349520e+00 -2.29244308e+00 -1.90326027e+00]\n",
            "  [-1.58768598e+00 -1.87232768e-01 -1.58318353e+00 -1.40414113e+00]\n",
            "  [-1.05082665e+00  1.36232050e+00 -1.33400317e+00 -1.13679257e+00]\n",
            "  [-7.95916000e-01  9.10211709e-01 -9.16933675e-01 -7.58591894e-01]\n",
            "  [-4.40461000e-01 -5.66937799e-02 -6.04889799e-01 -3.93238000e-01]]\n",
            "\n",
            " [[-1.61936217e+00 -1.56508795e+00 -1.62954229e+00 -1.19544660e+00]\n",
            "  [-1.29584779e+00 -6.59286846e-01 -1.36402719e+00  3.49249781e+00]\n",
            "  [-9.38050543e-01  1.19006853e+01 -5.98597920e-01  1.86400491e+00]\n",
            "  [-5.76275933e-01  1.64535340e+01  7.93407456e-01  1.02902245e+00]\n",
            "  [-3.93235804e-01  1.70347299e+01 -1.48411341e-01 -2.97010000e-01]]\n",
            "\n",
            " [[-1.17372896e+00 -1.11745519e+00 -1.20500063e+00 -9.57550802e-01]\n",
            "  [-1.14372914e+00 -9.13316549e-01 -8.03465495e-01  5.22380442e+00]\n",
            "  [-1.78358432e-01  2.47107625e-01 -6.85741533e-01  2.85927240e+01]\n",
            "  [ 3.24323582e+00  1.54330971e+01  1.95128550e+00  5.07060623e+01]\n",
            "  [ 8.09415214e-01  7.56684605e+01  4.24347887e+00  2.18780073e+01]]\n",
            "\n",
            " [[-8.31590666e-01 -7.96498313e-01 -7.42112189e-01 -7.95497137e-01]\n",
            "  [-7.17644704e-01 -3.28037728e-01 -6.03270846e-01 -5.30152292e-01]\n",
            "  [-5.09001611e-01  6.19060418e+00 -3.13763149e-01  5.78913358e+00]\n",
            "  [ 1.13245422e+01  5.21439288e+01 -2.07100000e-01  6.75108868e+00]\n",
            "  [ 8.66990810e+00  9.52898713e+01  1.04646031e+01  3.39645592e+01]]\n",
            "\n",
            " [[-5.98653943e-01 -5.52888271e-01 -7.29070141e-01 -3.79459959e-01]\n",
            "  [-4.89365607e-01 -2.88100000e-01 -5.10812578e-01  4.55761141e+00]\n",
            "  [-2.97010000e-01 -1.63441000e-01 -1.99000000e-01  4.66434586e+01]\n",
            "  [ 7.63641009e+00  2.05734777e+01  1.05498338e+01  8.90581011e+01]\n",
            "  [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agente Q-Learning : Michi o 3 en Raya\n",
        "\n",
        "1. Define el Entorno\n"
      ],
      "metadata": {
        "id": "Vze6JZLJhEHY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Definir el juego de Michi o 3 en raya\n",
        "class Michi:\n",
        "   def __init__(self):\n",
        "       self.board = [' ' for _ in range(9)]  # Tablero 3x3 (9 posiciones)\n",
        "       self.current_winner = None  # Ganador actual\n",
        "\n",
        "   def print_board(self):\n",
        "       for row in [self.board[i*3:(i+1)*3] for i in range(3)]:\n",
        "           print('| ' + ' | '.join(row) + ' |')\n",
        "\n",
        "   @staticmethod\n",
        "   def print_board_nums():\n",
        "       # Numeración del tablero (para que los jugadores sepan dónde jugar)\n",
        "       number_board = [[str(i) for i in range(j*3, (j+1)*3)] for j in range(3)]\n",
        "       for row in number_board:\n",
        "           print('| ' + ' | '.join(row) + ' |')\n",
        "\n",
        "   def available_moves(self):\n",
        "       return [i for i, x in enumerate(self.board) if x == ' ']\n",
        "\n",
        "   def empty_squares(self):\n",
        "       return ' ' in self.board\n",
        "\n",
        "   def num_empty_squares(self):\n",
        "       return self.board.count(' ')\n",
        "\n",
        "   def make_move(self, square, letter):\n",
        "       # Si el movimiento es válido, lo realiza y regresa True. Si no, False.\n",
        "       if self.board[square] == ' ':\n",
        "           self.board[square] = letter\n",
        "           if self.winner(square, letter):\n",
        "               self.current_winner = letter\n",
        "           return True\n",
        "       return False\n",
        "\n",
        "   def winner(self, square, letter):\n",
        "       # Chequea si el jugador ha ganado\n",
        "       # Ganar en fila\n",
        "       row_ind = square // 3\n",
        "       row = self.board[row_ind*3:(row_ind+1)*3]\n",
        "       if all([spot == letter for spot in row]):\n",
        "           return True\n",
        "       # Ganar en columna\n",
        "       col_ind = square % 3\n",
        "       column = [self.board[col_ind+i*3] for i in range(3)]\n",
        "       if all([spot == letter for spot in column]):\n",
        "           return True\n",
        "       # Ganar en diagonal\n",
        "       if square % 2 == 0:\n",
        "           diagonal1 = [self.board[i] for i in [0, 4, 8]]  # Diagonal de izquierda a derecha\n",
        "           if all([spot == letter for spot in diagonal1]):\n",
        "               return True\n",
        "           diagonal2 = [self.board[i] for i in [2, 4, 6]]  # Diagonal de derecha a izquierda\n",
        "           if all([spot == letter for spot in diagonal2]):\n",
        "               return True\n",
        "       return False\n"
      ],
      "metadata": {
        "id": "MXilco4ihCDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define al Agente"
      ],
      "metadata": {
        "id": "90GqwtvYjNjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el Agente basado en Q-learning\n",
        "class QLearningAgent:\n",
        "   def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
        "       self.q_table = {}  # Tabla Q\n",
        "       self.alpha = alpha  # Tasa de aprendizaje\n",
        "       self.gamma = gamma  # Factor de descuento\n",
        "       self.epsilon = epsilon  # Parámetro de exploración\n",
        "\n",
        "   def get_q_value(self, state, action):\n",
        "       return self.q_table.get((state, action), 0.0)\n",
        "\n",
        "   def update_q_value(self, state, action, reward, next_state):\n",
        "       old_q_value = self.q_table.get((state, action), 0.0)\n",
        "       future_q_values = [self.q_table.get((next_state, a), 0.0) for a in range(9)]\n",
        "       new_q_value = old_q_value + self.alpha * (reward + self.gamma * max(future_q_values) - old_q_value)\n",
        "       self.q_table[(state, action)] = new_q_value\n",
        "\n",
        "   def choose_action(self, state, available_moves):\n",
        "       if random.random() < self.epsilon:\n",
        "           (self.epsilon <= 0.1) and print(\"Explorando ...\")\n",
        "           return random.choice(available_moves)  # Explorar\n",
        "       else:\n",
        "           (self.epsilon <= 0.1) and print(\"Explotando %s\"%{a:round(self.get_q_value(state, a),2) for a in available_moves})\n",
        "           q_values = [self.get_q_value(state, action) for action in available_moves]\n",
        "           max_q_value = max(q_values)\n",
        "           available_index = [i for i in range(len(q_values)) if q_values[i] == max_q_value]\n",
        "           return available_moves[random.choice(available_index)]  # Explotar\n"
      ],
      "metadata": {
        "id": "qcfNWSUohZtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar el agente\n",
        "def train_agent(agent, episodes=10000):\n",
        "   for _ in range(episodes):\n",
        "       game = Michi()\n",
        "       state = tuple(game.board)\n",
        "       turno = True\n",
        "       while game.empty_squares():\n",
        "           turno = not turno\n",
        "           available_moves = game.available_moves()\n",
        "           action = agent.choose_action(state, available_moves)\n",
        "           game.make_move(action, turno and 'X' or 'O')\n",
        "           if game.current_winner == 'X':\n",
        "               agent.update_q_value(state, action, 1, None)  # Gana - Recompensa\n",
        "               break\n",
        "           elif game.current_winner == 'O':\n",
        "               agent.update_q_value(state, action, 0, None)  # Pierde - Castigo\n",
        "               break\n",
        "           elif game.num_empty_squares() == 0:\n",
        "               agent.update_q_value(state, action, 0.5, None)  # Empate\n",
        "               break\n",
        "           else:\n",
        "               next_state = tuple(game.board)\n",
        "               agent.update_q_value(state, action, 0, next_state)\n",
        "               state = next_state"
      ],
      "metadata": {
        "id": "1meOiku3hd3i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define el Juego"
      ],
      "metadata": {
        "id": "RLQZf70tj1aO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Jugar contra el agente\n",
        "def play_against_agent(agent):\n",
        "   game = Michi()\n",
        "   game.print_board_nums()\n",
        "   while game.empty_squares():\n",
        "       if game.num_empty_squares() % 2 == 1:  # Turno del humano\n",
        "           square = int(input('Selecciona un movimiento (0-8): '))\n",
        "           if game.make_move(square, 'O'):\n",
        "               game.print_board()\n",
        "               if game.current_winner:\n",
        "                   print('¡Ganaste!')\n",
        "                   return\n",
        "       else:  # Turno del agente\n",
        "           state = tuple(game.board)\n",
        "           available_moves = game.available_moves()\n",
        "           action = agent.choose_action(state, available_moves)\n",
        "           print(f\"state: {state} -> action: {action}\\n\")\n",
        "           game.make_move(action, 'X')\n",
        "           game.print_board()\n",
        "           if game.current_winner:\n",
        "               print('¡El agente gana!')\n",
        "               return\n",
        "   print('¡Empate!')\n"
      ],
      "metadata": {
        "id": "IoiKR9juhjEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Entrenamiento"
      ],
      "metadata": {
        "id": "w4JZifbQkLNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos el agente y luego jugamos\n",
        "agent = QLearningAgent()\n",
        "agent.epsilon = 0.5  # Aumentar la exploración\n",
        "\n",
        "train_agent(agent, episodes=100000)\n",
        "print(\"Agente Entrenado ...\")\n",
        "\n",
        "agent.epsilon = 0.1  # Restablece la exploración"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIatn9wNhm7B",
        "outputId": "b31e153e-51d7-42c9-b9ca-cd2753585e14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agente Entrenado ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Jugar"
      ],
      "metadata": {
        "id": "WtKDsaK0lKO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "play_against_agent(agent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHNYeWQFlL5r",
        "outputId": "d27e29e6-99f4-4880-9249-d8e2144c59e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 0 | 1 | 2 |\n",
            "| 3 | 4 | 5 |\n",
            "| 6 | 7 | 8 |\n",
            "Selecciona un movimiento (0-8): 2\n",
            "|   |   | O |\n",
            "|   |   |   |\n",
            "|   |   |   |\n",
            "Explotando {0: 0.66, 1: 0.66, 3: 0.66, 4: 0.66, 5: 0.66, 6: 0.66, 7: 0.66, 8: 0.66}\n",
            "state: (' ', ' ', 'O', ' ', ' ', ' ', ' ', ' ', ' ') -> action: 4\n",
            "\n",
            "|   |   | O |\n",
            "|   | X |   |\n",
            "|   |   |   |\n",
            "Selecciona un movimiento (0-8): 0\n",
            "| O |   | O |\n",
            "|   | X |   |\n",
            "|   |   |   |\n",
            "Explotando {1: 0.81, 3: 0.32, 5: 0.79, 6: 0.35, 7: 0.63, 8: 0.48}\n",
            "state: ('O', ' ', 'O', ' ', 'X', ' ', ' ', ' ', ' ') -> action: 1\n",
            "\n",
            "| O | X | O |\n",
            "|   | X |   |\n",
            "|   |   |   |\n",
            "Selecciona un movimiento (0-8): 7\n",
            "| O | X | O |\n",
            "|   | X |   |\n",
            "|   | O |   |\n",
            "Explotando {3: 0.47, 5: 0.23, 6: 0.01, 8: 0.14}\n",
            "state: ('O', 'X', 'O', ' ', 'X', ' ', ' ', 'O', ' ') -> action: 3\n",
            "\n",
            "| O | X | O |\n",
            "| X | X |   |\n",
            "|   | O |   |\n",
            "Selecciona un movimiento (0-8): 6\n",
            "| O | X | O |\n",
            "| X | X |   |\n",
            "| O | O |   |\n",
            "Explorando ...\n",
            "state: ('O', 'X', 'O', 'X', 'X', ' ', 'O', 'O', ' ') -> action: 5\n",
            "\n",
            "| O | X | O |\n",
            "| X | X | X |\n",
            "| O | O |   |\n",
            "¡El agente gana!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agent.q_table"
      ],
      "metadata": {
        "id": "6XnmUm9aie4j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}