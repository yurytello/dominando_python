# -*- coding: utf-8 -*-
"""11 Aprendizaje Reforzado.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-O-dRwnhF-fPHmPJASjlSohtduIz0HgU

## Implementaión de la Q-Table

### Problema: Agente en un Laberinto
El agente se encuentra en un entorno de cuadrícula (5x5) y debe aprender a llegar a una meta. La recompensa se otorga cuando llega a la meta, y hay penalizaciones por cada paso que da.

Entorno (Grid World):
El entorno es una cuadrícula en la que:

- El agente comienza en la posición (0, 0).
- La meta está en la posición (4, 4).
- Cada acción tiene un costo de -1 para incentivar llegar rápidamente a la meta.
- Al llegar a la meta, el agente recibe una recompensa de +100.

Acciones:
El agente tiene 4 posibles acciones:

- Arriba
- Abajo
- Izquierda
- Derecha
"""

import numpy as np
import random

# Parámetros del entorno
rows, cols = 5, 5  # Tamaño del entorno 5x5
goal = (4, 4)      # La meta está en la posición (4, 4)

# Parámetros del Q-Learning
alpha = 0.1  # Tasa de aprendizaje
gamma = 0.9  # Factor de descuento
epsilon = 0.4  # Parámetro de exploración (probabilidad de tomar acción aleatoria)
num_episodes = 50  # Número de episodios para entrenar

# Inicializar la tabla Q con ceros
Q_table = np.zeros((rows, cols, 4))  # 4 acciones: arriba, abajo, izquierda, derecha

# Función para elegir la acción (política epsilon-greedy)
def choose_action(state):
    if random.uniform(0, 1) < epsilon:  # Exploración
        return random.choice([0, 1, 2, 3])  # Arriba, Abajo, Izquierda, Derecha
    else:  # Explotación
        return np.argmax(Q_table[state[0], state[1]])

# Función para realizar la acción y actualizar el estado
def take_action(state, action):
    row, col = state
    if action == 0 and row > 0:         # Arriba
        row -= 1
    elif action == 1 and row < rows - 1:  # Abajo
        row += 1
    elif action == 2 and col > 0:       # Izquierda
        col -= 1
    elif action == 3 and col < cols - 1:  # Derecha
        col += 1
    return (row, col)

# Función de entrenamiento usando Q-Learning
for episode in range(num_episodes):
    state = (0, 0)  # El agente siempre empieza en (0, 0)
    total_reward = 0
    done = False
    steps = 0

    while not done:
        action = choose_action(state)  # Escoge una acción

        next_state = take_action(state, action)  # Mueve al agente
        reward = -1  # Penalización por movimiento

        if next_state == goal:  # Si el agente llega a la meta
            reward = 100
            done = True

        # Actualización de la tabla Q
        old_value = Q_table[state[0], state[1], action]
        next_max = np.max(Q_table[next_state[0], next_state[1]])

        # Fórmula de actualización de Q
        Q_table[state[0], state[1], action] = old_value + alpha * (reward + gamma * next_max - old_value)

        state = next_state  # Mover al agente al siguiente estado
        total_reward += reward
        steps += 1
        print(f"{state}", end=' ')

    # Al final de cada episodio, se puede imprimir el progreso
    print(f"\nEpisodio {episode + 1}, Recompensa total: {total_reward}, Pasos: {steps}")

# Mostrar la tabla Q final
print("Tabla Q entrenada:")
print(Q_table)

"""## Agente Q-Learning : Michi o 3 en Raya

1. Define el Entorno

"""

import numpy as np
import random

# Definir el juego de Michi o 3 en raya
class Michi:
   def __init__(self):
       self.board = [' ' for _ in range(9)]  # Tablero 3x3 (9 posiciones)
       self.current_winner = None  # Ganador actual

   def print_board(self):
       for row in [self.board[i*3:(i+1)*3] for i in range(3)]:
           print('| ' + ' | '.join(row) + ' |')

   @staticmethod
   def print_board_nums():
       # Numeración del tablero (para que los jugadores sepan dónde jugar)
       number_board = [[str(i) for i in range(j*3, (j+1)*3)] for j in range(3)]
       for row in number_board:
           print('| ' + ' | '.join(row) + ' |')

   def available_moves(self):
       return [i for i, x in enumerate(self.board) if x == ' ']

   def empty_squares(self):
       return ' ' in self.board

   def num_empty_squares(self):
       return self.board.count(' ')

   def make_move(self, square, letter):
       # Si el movimiento es válido, lo realiza y regresa True. Si no, False.
       if self.board[square] == ' ':
           self.board[square] = letter
           if self.winner(square, letter):
               self.current_winner = letter
           return True
       return False

   def winner(self, square, letter):
       # Chequea si el jugador ha ganado
       # Ganar en fila
       row_ind = square // 3
       row = self.board[row_ind*3:(row_ind+1)*3]
       if all([spot == letter for spot in row]):
           return True
       # Ganar en columna
       col_ind = square % 3
       column = [self.board[col_ind+i*3] for i in range(3)]
       if all([spot == letter for spot in column]):
           return True
       # Ganar en diagonal
       if square % 2 == 0:
           diagonal1 = [self.board[i] for i in [0, 4, 8]]  # Diagonal de izquierda a derecha
           if all([spot == letter for spot in diagonal1]):
               return True
           diagonal2 = [self.board[i] for i in [2, 4, 6]]  # Diagonal de derecha a izquierda
           if all([spot == letter for spot in diagonal2]):
               return True
       return False

"""2. Define al Agente"""

# Definir el Agente basado en Q-learning
class QLearningAgent:
   def __init__(self, alpha=0.1, gamma=0.9, epsilon=0.1):
       self.q_table = {}  # Tabla Q
       self.alpha = alpha  # Tasa de aprendizaje
       self.gamma = gamma  # Factor de descuento
       self.epsilon = epsilon  # Parámetro de exploración

   def get_q_value(self, state, action):
       return self.q_table.get((state, action), 0.0)

   def update_q_value(self, state, action, reward, next_state):
       old_q_value = self.q_table.get((state, action), 0.0)
       future_q_values = [self.q_table.get((next_state, a), 0.0) for a in range(9)]
       new_q_value = old_q_value + self.alpha * (reward + self.gamma * max(future_q_values) - old_q_value)
       self.q_table[(state, action)] = new_q_value

   def choose_action(self, state, available_moves):
       if random.random() < self.epsilon:
           (self.epsilon <= 0.1) and print("Explorando ...")
           return random.choice(available_moves)  # Explorar
       else:
           (self.epsilon <= 0.1) and print("Explotando %s"%{a:round(self.get_q_value(state, a),2) for a in available_moves})
           q_values = [self.get_q_value(state, action) for action in available_moves]
           max_q_value = max(q_values)
           available_index = [i for i in range(len(q_values)) if q_values[i] == max_q_value]
           return available_moves[random.choice(available_index)]  # Explotar

# Función para entrenar el agente
def train_agent(agent, episodes=10000):
   for _ in range(episodes):
       game = Michi()
       state = tuple(game.board)
       turno = True
       while game.empty_squares():
           turno = not turno
           available_moves = game.available_moves()
           action = agent.choose_action(state, available_moves)
           game.make_move(action, turno and 'X' or 'O')
           if game.current_winner == 'X':
               agent.update_q_value(state, action, 1, None)  # Gana - Recompensa
               break
           elif game.current_winner == 'O':
               agent.update_q_value(state, action, 0, None)  # Pierde - Castigo
               break
           elif game.num_empty_squares() == 0:
               agent.update_q_value(state, action, 0.5, None)  # Empate
               break
           else:
               next_state = tuple(game.board)
               agent.update_q_value(state, action, 0, next_state)
               state = next_state

"""3. Define el Juego"""

# Jugar contra el agente
def play_against_agent(agent):
   game = Michi()
   game.print_board_nums()
   while game.empty_squares():
       if game.num_empty_squares() % 2 == 1:  # Turno del humano
           square = int(input('Selecciona un movimiento (0-8): '))
           if game.make_move(square, 'O'):
               game.print_board()
               if game.current_winner:
                   print('¡Ganaste!')
                   return
       else:  # Turno del agente
           state = tuple(game.board)
           available_moves = game.available_moves()
           action = agent.choose_action(state, available_moves)
           print(f"state: {state} -> action: {action}\n")
           game.make_move(action, 'X')
           game.print_board()
           if game.current_winner:
               print('¡El agente gana!')
               return
   print('¡Empate!')

"""4. Entrenamiento"""

# Entrenamos el agente y luego jugamos
agent = QLearningAgent()
agent.epsilon = 0.5  # Aumentar la exploración

train_agent(agent, episodes=100000)
print("Agente Entrenado ...")

agent.epsilon = 0.1  # Restablece la exploración

"""5. Jugar"""

play_against_agent(agent)

agent.q_table